{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Opts():\n",
    "    def __init__(self):\n",
    "        self.train_data_path = \"./working/train/\"\n",
    "        self.test_data_path = \"./working/test/\"\n",
    "        self.csv_path = \"./meta\"\n",
    "        self.data_path = \"./data/\"\n",
    "        self.model_save_path = \"./output/\"\n",
    "        self.model_name = \"cnn.h5\"\n",
    "        self.layer_name = \"conv2d_20\"\n",
    "        self.plot_save_path = \"./plot/cnn\"\n",
    "        \n",
    "        if not os.path.exists(self.plot_save_path):\n",
    "            os.mkdir(self.plot_save_path)\n",
    "opts = Opts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n",
    "\n",
    "\n",
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(412, 412),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_filter_image(input_img,\n",
    "                               layer_output,\n",
    "                               filter_index):\n",
    "        \"\"\"Generates image for one particular filter.\n",
    "\n",
    "        # Arguments\n",
    "            input_img: The input-image Tensor.\n",
    "            layer_output: The output-image Tensor.\n",
    "            filter_index: The to be processed filter number.\n",
    "                          Assumed to be valid.\n",
    "\n",
    "        #Returns\n",
    "            Either None if no image could be generated.\n",
    "            or a tuple of the image (array) itself and the last loss.\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random(\n",
    "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "        else:\n",
    "            input_img_data = np.random.random(\n",
    "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # Slowly upscaling towards the original size prevents\n",
    "        # a dominating high-frequency of the to visualized structure\n",
    "        # as it would occur if we directly compute the 412d-image.\n",
    "        # Behaves as a better starting point for each following dimension\n",
    "        # and therefore avoids poor local minima\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # we run gradient ascent for e.g. 20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value * step\n",
    "\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                if loss_value <= K.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # Calculate upscaled dimension\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # Upscale\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = np.expand_dims(\n",
    "                process_image(img, input_img_data[0]), 0)\n",
    "\n",
    "        # decode the resulting input image\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  loss_value,\n",
    "                                                                  e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"Draw the best filters in a nxn grid.\n",
    "\n",
    "        # Arguments\n",
    "            filters: A List of generated images and their corresponding losses\n",
    "                     for each processed filter.\n",
    "            n: dimension of the grid.\n",
    "               If none, the largest possible square will be used\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # the filters that have the highest loss are assumed to be better-looking.\n",
    "        # we will only keep the top n*n filters.\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # build a black picture with enough space for\n",
    "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # fill the picture with our saved filters\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "    # this is the placeholder for the input images\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "    output_layer = layer_dict[layer_name]\n",
    "    assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "    # Compute to be processed filter range\n",
    "    filter_lower = filter_range[0]\n",
    "    filter_upper = (filter_range[1]\n",
    "                    if filter_range[1] is not None\n",
    "                    else len(output_layer.get_weights()[1]))\n",
    "    assert(filter_lower >= 0\n",
    "           and filter_upper <= len(output_layer.get_weights()[1])\n",
    "           and filter_upper > filter_lower)\n",
    "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "    # iterate through each filter and generate its corresponding image\n",
    "    processed_filters = []\n",
    "    for f in range(filter_lower, filter_upper):\n",
    "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "        if img_loss is not None:\n",
    "            processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # Finally draw and store the best filters to disk\n",
    "    _draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(opts.model_save_path, opts.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_24': <keras.layers.core.Activation at 0xb27d1a828>,\n",
       " 'activation_25': <keras.layers.core.Activation at 0xb27dc0860>,\n",
       " 'activation_26': <keras.layers.core.Activation at 0xb27d1a7f0>,\n",
       " 'activation_27': <keras.layers.core.Activation at 0xb27e3cf60>,\n",
       " 'activation_28': <keras.layers.core.Activation at 0xb27e68be0>,\n",
       " 'activation_29': <keras.layers.core.Activation at 0xb27eb2f98>,\n",
       " 'activation_30': <keras.layers.core.Activation at 0xb27f25828>,\n",
       " 'conv2d_20': <keras.layers.convolutional.Conv2D at 0xb27d1a908>,\n",
       " 'conv2d_21': <keras.layers.convolutional.Conv2D at 0xb27ddc160>,\n",
       " 'conv2d_22': <keras.layers.convolutional.Conv2D at 0xb27df1320>,\n",
       " 'conv2d_23': <keras.layers.convolutional.Conv2D at 0xb27d15cc0>,\n",
       " 'conv2d_24': <keras.layers.convolutional.Conv2D at 0xb27e68d68>,\n",
       " 'dense_7': <keras.layers.core.Dense at 0xb27edcda0>,\n",
       " 'dense_8': <keras.layers.core.Dense at 0xb27f259e8>,\n",
       " 'dropout_13': <keras.layers.core.Dropout at 0xb27ddcf98>,\n",
       " 'dropout_14': <keras.layers.core.Dropout at 0x106c97320>,\n",
       " 'dropout_15': <keras.layers.core.Dropout at 0xb27ec5198>,\n",
       " 'dropout_16': <keras.layers.core.Dropout at 0xb27f25908>,\n",
       " 'flatten_4': <keras.layers.core.Flatten at 0xb27ec5c18>,\n",
       " 'max_pooling2d_10': <keras.layers.pooling.MaxPooling2D at 0xb27dc0898>,\n",
       " 'max_pooling2d_11': <keras.layers.pooling.MaxPooling2D at 0xb27e3cda0>,\n",
       " 'max_pooling2d_12': <keras.layers.pooling.MaxPooling2D at 0xb27eb2dd8>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute filters 0 to 64\n",
      "Costs of filter   2:     7 ( 11.44s )\n",
      "Costs of filter   3:    12 ( 10.92s )\n",
      "Costs of filter   4:    15 ( 10.87s )\n",
      "Costs of filter   5:    14 ( 10.53s )\n",
      "Costs of filter   6:    14 ( 10.50s )\n",
      "Costs of filter  11:    21 ( 10.50s )\n",
      "Costs of filter  12:     7 ( 10.56s )\n",
      "Costs of filter  14:    15 ( 10.51s )\n",
      "Costs of filter  15:    10 ( 10.32s )\n",
      "Costs of filter  16:    11 ( 10.84s )\n",
      "Costs of filter  20:    11 ( 10.91s )\n",
      "Costs of filter  23:    11 ( 10.53s )\n",
      "Costs of filter  24:     5 ( 10.52s )\n",
      "Costs of filter  26:    10 ( 10.56s )\n",
      "Costs of filter  27:     2 ( 10.86s )\n",
      "Costs of filter  29:     5 ( 11.75s )\n",
      "Costs of filter  31:    15 ( 12.42s )\n",
      "Costs of filter  32:    11 ( 12.80s )\n",
      "Costs of filter  34:    12 ( 11.30s )\n",
      "Costs of filter  35:    11 ( 11.01s )\n",
      "Costs of filter  37:     7 ( 11.84s )\n",
      "Costs of filter  43:     7 ( 11.62s )\n",
      "Costs of filter  45:    12 ( 11.65s )\n",
      "Costs of filter  46:    15 ( 12.60s )\n",
      "Costs of filter  47:     5 ( 13.26s )\n",
      "Costs of filter  48:    12 ( 11.80s )\n",
      "Costs of filter  51:     8 ( 10.61s )\n",
      "Costs of filter  52:    10 ( 10.90s )\n",
      "Costs of filter  55:    15 ( 12.37s )\n",
      "Costs of filter  56:     7 ( 11.70s )\n",
      "Costs of filter  58:    10 ( 12.36s )\n",
      "Costs of filter  59:    16 ( 12.12s )\n",
      "Costs of filter  61:     4 ( 13.30s )\n",
      "Costs of filter  62:     4 ( 14.91s )\n",
      "Costs of filter  63:     6 ( 14.32s )\n",
      "35 filter processed.\n"
     ]
    }
   ],
   "source": [
    "# example function call\n",
    "visualize_layer(model, opts.layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "test_csv_path = os.path.join(opts.csv_path, \"test.csv\")\n",
    "test_file_df = df_from_csv(test_csv_path)\n",
    "\n",
    "def append_ext(fn):\n",
    "    return fn+\".jpg\"\n",
    "test_file_df[\"file_names\"] = test_file_df[\"file_names\"].apply(append_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 925 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1./255.)\n",
    "test_gen=test_data_gen.flow_from_dataframe(\n",
    "    dataframe=test_file_df,\n",
    "    directory=opts.test_data_path,\n",
    "    x_col=\"file_names\",\n",
    "    y_col=\"class_names\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(64,64))\n",
    "STEP_SIZE_TEST=test_gen.n//test_gen.batch_size\n",
    "test_gen.reset() # resets batch index to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 10s 362ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict_generator(test_gen,\n",
    "                             steps=STEP_SIZE_TEST,\n",
    "                             verbose=1)\n",
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAEWCAYAAAD4hSV+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['AC', 'CH', 'CP', 'DB', 'DR', 'EI', 'GS', 'JH', 'SI', 'SM']\n",
    "cm = confusion_matrix([labels[i] for i in test_gen.classes[:896]], \\\n",
    "                      [labels[j] for j in predicted_class_indices], \\\n",
    "                      labels)\n",
    "\n",
    "# plt.imshow(cm)\n",
    "# plt.xlabel(\"Predicted labels\")\n",
    "# plt.ylabel(\"True labels\")\n",
    "\n",
    "# plt.xticks(np.arange(0,10),labels)\n",
    "# plt.yticks(np.arange(0,10),labels)\n",
    "# plt.title('Confusion Matrix of Baseline CNN')\n",
    "# plt.colorbar()\n",
    "# plt.savefig(os.path.join(opts.plot_save_path, \"cm(baseline)\"), bbox_inches='tight')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "#Fetch labels from train gen for testing\n",
    "# labels = (train_gen.class_indices)\n",
    "# labels = dict((v,k) for k,v in labels.items())\n",
    "# predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          opts,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "    Usage\n",
    "        -----\n",
    "        plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                                  # sklearn.metrics.confusion_matrix\n",
    "                              normalize    = True,                # show proportions\n",
    "                              target_names = y_labels_vals,       # list of names of the classes\n",
    "                              title        = best_estimator_name) # title of graph\n",
    "\n",
    "        Citiation\n",
    "        ---------\n",
    "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "        \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig(os.path.join(opts.plot_save_path, title), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_confusion_matrix() got multiple values for argument 'target_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-28726c6d7706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                       \u001b[0mnormalize\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                       title        = \"Confusion Matrix of Baseline CNN\")\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_confusion_matrix() got multiple values for argument 'target_names'"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(cm, \n",
    "                      opts,\n",
    "                      labels,\n",
    "                      normalize = False,\n",
    "                      title = \"Confusion Matrix of Baseline CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,   0,   9,   3,   2,  34,   3,  12,  23,   0],\n",
       "       [  0,  33,   0,   3,   4,   0,   0,   1,   0,   0],\n",
       "       [  2,   2,  45,  29,   2,   1,   8,   0,   2,   6],\n",
       "       [  0,   0,   8,  75,   0,   3,   4,   1,   2,   4],\n",
       "       [  0,   9,   4,   7,  54,   2,   1,   9,   6,   5],\n",
       "       [  5,   0,   4,   2,   2,  34,   0,  44,   2,  11],\n",
       "       [  0,   0,   3,   1,   4,   0,  21,   0,   0,   6],\n",
       "       [ 23,   6,   5,   0,  23,  17,   0,  35,   1,   8],\n",
       "       [  0,   0,   2,   0,   0,   1,   0,   7, 101,   4],\n",
       "       [  5,   0,  25,   3,  12,   6,   0,   4,   4,  37]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
